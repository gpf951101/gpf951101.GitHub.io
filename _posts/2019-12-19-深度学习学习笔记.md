---
layout: article
title: 神经网络学习笔记
date: 2019-12-19
tages: [神经网络]
sharing: true
show_author_profile: true
lang: zh
comment: true
pageview: true
---

参考地址：
[反向神经网络](https://www.cnblogs.com/charlotte77/p/5629865.html)
[CNN](https://www.cnblogs.com/charlotte77/p/7759802.html)
[RNN](https://blog.csdn.net/qq_39422642/article/details/78676567)
[LSTM](https://blog.csdn.net/weixin_42111770/article/details/80900575)
<!--more-->

# 反向神经网络

1. 输入输出一样，即为自编码模型。(Auto-Encoder)
    - 应用：图像识别、文本分类

2. 反向神经网络的自我理解：
    - 目标：总误差最小
    - 方法：求导。求出对每个权重的偏导，实际是有具体数值输入的，所以最后通过偏导公式可以求出具体的数值。求导是一个链式求导，不断的向前，直到到达输入层。
    - 通过学习率，更新权重值。更新学习率就是用当前值减去总误差对当前权重的偏导与学习率的乘积，最后的差值作为新的权重值。

# CNN

## 传统神经网络

### 1.神经网络与机器学习的比较
    - 机器学习需要明确feature和label。feature过少，欠拟合，feature过多，过拟合。神经网络不需要做特别大的特征工程。
    - 机器学习的输入数据需要进行很多的预处理，如归一化等，神经网络不需要过多处理。
    - 对于调参不需要特别深厚的理论知识。

### 2.缺点
    - 网络层越深，参数会非常多，模型越复杂，越不好调参，容易出现过拟合。
    - 不断的迭代会导致梯度越来越小，如果趋于0的话，则权重无法更新。

## 卷积神经网络（CNN）

### 1.传统神经网络的缺点
    - 对于输入为28*28的手写数字识别来说，输入为28*28，输出为10，15个隐含层神经元，即需要28*28*15*10+15+10个参数，参数巨大。

### 2.卷积神经网络

#### 2.1 三个基本层

##### 2.1.1 卷积层（Convolutional Layer）

**局部特征**

可以把卷积层理解成特征提取器。
*是否可理解为降维呢？*

1. 卷积运算有多种，最简单的卷积运算是两个卷积核大小的矩阵的内积运算，即相同位置的数字相乘再加求和。
2. 卷积核的大小和个数可以自己定义。一般情况，根据实验得到的经验来看，会在越靠近输入的卷积层设定少量的卷积核，越往后，卷积层设定的卷积核数目就越多。

##### 2.1.2 池化层

- 目的：通过降采样的方式，在不影响图像质量的情况下，压缩图片，减少参数。
- 方法：
    - MaxPooling：取滑动窗口里的最大值
    - AveragePooling：取滑动窗口内所有值的平均值

**为什么需要MaxPooling？**
- 卷积核负责提取不同的特征，对其进行MaxPooling操作后，提取出的是真正能够是呗特征的数值，其余被舍弃的数值，对于提取特定的特征并没有特别大的帮助。在后续计算中减小feature map的尺寸，从而减少参数，达到减少计算量，损失效果小的结果。
- 并不是所有情况Max Pooling效果都好，有时候周边信息也会对某个特定特征的识别产生一定效果，此时不能舍弃该信息，需视具体情况决定。


**Zero Padding**

通过卷积层、池化层的不断处理，数据大小会变得越来越小，所以有Zero Padding操作，帮助我们保证每次经过卷积或者池化输出后的图片的大小不变。

先补零，再卷积。

3*3的卷积核和1的zero Padding，或者5*5的卷积核和2的zero padding，保留图片的原始尺寸。

默认是正方形的卷积核，若两者不等，可分开计算，分开补零。

##### Flatten

做完Max Pooling后，把数据变为一维向量，放到Flatten层，把Flatten层的output放到full connected Layer里，采用sofrmax进行分类。

# RNN

## 与人工神经网络和卷积神经网络的区别

- 人工神经网络和卷积神经网络都是假设元素之间是相互独立的，输入输出也是相互独立的。
- 现实中，很多元素是相互联系的。
- 循环神经网络的本质是：**像人一样拥有记忆**。因此RNN的输入输出依赖当前的输入和记忆。

## 网络结构和原理

结构图如下：
![RNN结构图](/images/20191220114223.png)

# LSTM

## 基础

1. LSTM，长短期记忆网络，是一种时间递归神经网络，适合于处理和预测时间序列中间隔和延迟相对较长的重要事件。
2. 解决RNN中存在的梯度消失问题，是特殊的循环神经网络。
3. 门结构：输入门，遗忘门和输出门。
![结构图](/images/20191222185410.png)
4. 在LSTM中，第一阶段是遗忘门，决定哪些信息需要从细胞状态中被遗忘，下一阶段是输入门，确定那些信息能够存档到细胞状态中，最后一个阶段是输出门，输出门确定输出什么值。

## LSTM门结构

1. 遗忘门：以上一层的输出和本层的输入的序列数据作为输入，通过激活函数，得到输出，范围是[0,1]，表示上一层细胞状态被遗忘的概率，1是完全保留就，0是完全舍弃。
2. 输入门包括两个部分，第一部分是sigmoid激活函数，第二部分是tanh函数，输出为两相乘，表示有多好啊新信息被保留。更新当前细胞状态。（原博客写的很好，可以跳转原文查看）
3. 输出门用来控制该层的细胞状态有多少被过滤。
[此处详细](https://blog.csdn.net/weixin_42111770/article/details/80900575)

## 双向LSTM
双向RNN由两个普通的RNN所组成，一个正向的RNN，利用过去的信息，一个逆序的RNN，利用未来的信息，这样在时刻t，既能够使用t-1时刻的信息，又能够利用到t+1时刻的信息。一般来说，由于双向LSTM能够同时利用过去时刻和未来时刻的信息，会比单向LSTM最终的预测更加准确。
